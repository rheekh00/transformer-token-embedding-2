# transformer-token-embedding-2

## 개요

이 저장소는 트랜스포머 모델의 토큰 임베딩 학습 과정을 분석한 연구 결과를 담고 있습니다. 특히, 토큰 임베딩 벡터의 변화와 데이터 등장 빈도, 데이터 중복 및 다양성이 임베딩에 미치는 영향을 실험적으로 확인하였습니다. 또한, 모델의 각 레이어가 감지하는 언어적 패턴의 차이를 토큰 임베딩 벡터의 관점에서 분석하였습니다.

## 연구 배경 및 동기

- **연구 배경**: 임베딩 테이블의 학습 과정과 동작 메커니즘에 대한 이해는 모델 성능과 일반화 개선에 필수적입니다. 토큰 간 유사성과 코사인 유사도의 변화는 학습 과정에서 중요한 단서를 제공합니다.
- **연구 동기**: 대규모 언어 모델(LLM)이 단어 사이의 관계를 학습하고 이해하는 과정을 파악하고, 토큰 임베딩 간의 코사인 유사도의 변화를 통해 임베딩 학습 방식을 이해하고자 합니다.
- **연구 목표**:
  - 학습 중 임베딩 벡터의 변화를 정량적으로 분석.
  - 토큰의 데이터 등장 빈도와 코사인 유사도 사이의 상관관계 탐구.
  - 데이터 중복과 다양성이 토큰 임베딩 벡터에 미치는 영향을 실험적으로 확인.
  - 각 레이어가 감지하는 언어적 패턴의 차이를 토큰 임베딩 벡터의 관점으로 확인.

## 실험 설정

### 데이터셋
- **Dataset**: Wikitext-103-v1
- **Number of Sentences**: 1,165,029
- **Data Size**: 1,165,029 tokens

### 모델 구조 (RoBERTa)
- Hidden Dimension (h_dim): 512
- Feedforward Dimension (ff_dim): 2048
- Number of Layers: 6, 12

### 훈련 설정
- Batch Size: 100 (effective per GPU batch size = 50, with 4 GPUs)
- Steps Per Epoch: 5825
- Number of Epochs: 40
- Total Steps: 233,000

## 주요 실험 결과

### 1. 토큰 임베딩 벡터의 학습 패턴
  ![Image](https://github.com/user-attachments/assets/6324bb08-ca62-4eb3-a391-891a4ab6c9fd)
  ![image](https://github.com/user-attachments/assets/8f00c2ad-15a7-4d80-b569-0d3095a0893a)<br><br>
- **전체 토큰 간 유사도 증가**: 학습이 진행됨에 따라 전체 토큰 간 코사인 유사도가 증가하는 경향을 보임.
- **자주 등장하는 토큰**: 다양한 문장의 언어적 정보를 담아 다른 토큰과의 코사인 유사도가 낮음.
- **드물게 등장하는 토큰**: 비슷한 방향으로 업데이트되어 유사도가 높음.

  
### 2. 데이터 등장 빈도와 코사인 유사도
  ![image](https://github.com/user-attachments/assets/38937561-042a-4771-aebc-83c2f8d6333b)
- **자주 등장하는 토큰**: 다른 토큰과의 코사인 유사도가 낮음.
- **드물게 등장하는 토큰**: 유사도가 높음.
- **토큰의 데이터 등장 빈도(로그 스케일)와 코사인 유사도 평균의 상관 계수
  - **Pearson correlation** = -0.2022 (약한 음의 선형관계)
  - **Spearman correlation** = -0.8044 (강한 비선형 음의 순위관계)<br><br>

  ![image](https://github.com/user-attachments/assets/c27ac0ee-0258-4cef-8e6f-191dd11ecf87)
- **Zero Count 토큰**: 훈련 데이터에서 한 번도 등장하지 않은 토큰들은 다른 임베딩 벡터들과 높은 유사도를 보임.

### 3. 데이터 중복과 다양성의 영향
- **데이터 중복**: 동일한 데이터를 단순 반복하여 학습시키는 것은 모델이 데이터를 암기하도록 유도하며, 일반화 성능 저하 및 과적합 문제를 초래.
- **데이터 다양성**: 고품질이거나 다양성을 가진 데이터의 반복은 모델의 사전 학습 효율성과 다운스트림 작업 성능을 개선 가능.

### 4. 토큰 임베딩 벡터와 Common Vector의 코사인 유사도
- **Common Vector**: 전체 임베딩 벡터의 평균 벡터.
- **학습 초기**: 코사인 유사도 증가.
- **학습 진행**: 문장 학습 시 진동 발생.
- **Zero Count 토큰**: Common Vector와 지속적으로 높은 유사도 유지.

### 5. 모델 파라미터의 임베딩 공간 분석
- **Lower Layer (Layer 0-6)**: 군집이 뚜렷하고 집중됨. 주로 문법적, 구문적 패턴을 학습.
- **Higher Layer (Layer 7-11)**: 데이터 포인트의 분포가 퍼져 있음. 고차원 의미적 패턴을 학습.

### 6. 임베딩 행렬 공유 실험
- **인코더와 디코더 임베딩 행렬 공유**: 인코더와 디코더의 임베딩 행렬을 공유했을 때의 성능을 비교.
- **결과**:
  - **파라미터 수 감소**: 임베딩 행렬을 공유함으로써 모델의 전체 파라미터 수가 약 20% 감소.
  - **성능 변화**:
    - **BLEU 점수**: 임베딩 행렬을 공유한 모델의 BLEU 점수가 약 1.5점 하락.
    - **번역 품질**: 특정 문맥에서의 번역 품질이 약간 저하되었으나, 일반적인 문장에서는 큰 차이가 없음.
  - **학습 속도**: 파라미터 수가 줄어들어 학습 속도가 약 15% 빨라짐.
- **결론**: 임베딩 행렬을 공유하면 모델의 파라미터 수가 줄어들어 계산 효율성이 향상되고 학습 속도가 빨라지는 장점이 있습니다. 그러나 이로 인해 모델의 표현력이 약간 제한되어 번역 품질이 약간 하락할 수 있습니다. 따라서, 모델의 크기와 계산 효율성을 고려할 때 임베딩 행렬 공유는 유용한 전략이 될 수 있지만, 번역 품질이 중요한 경우에는 공유하지 않는 것이 더 나을 수 있습니다.

### 7. 토큰 임베딩 벡터의 차원 축소 시각화
- **차원 축소 방법**:
  - 1차 차원 축소: PCA (n_component = 50)
  - 2차 차원 축소: t-SNE
- **결과**:
  - **군집 형성**: 의미적/문법적으로 같은 카테고리에 속하는 토큰들이 높은 코사인 유사도를 가짐.
    - 예시: 색상 ('red', 'green', 'blue'), 대명사 ('he', 'she', 'they'), 크기 ('big', 'small', 'large').
  - **반대 개념의 높은 유사도**: 'big'과 'small', 'liberal'과 'conservative' 등의 반대 개념도 높은 코사인 유사도를 가짐.
  - **월(month) 토큰의 군집화**: 'January', 'February', 'March' 등의 월 토큰들이 높은 유사도를 보이며 군집을 형성.

## 결론

이 연구는 트랜스포머 모델의 토큰 임베딩 학습 과정을 심층적으로 분석하였습니다. 특히, 데이터 등장 빈도, 데이터 중복 및 다양성이 임베딩 벡터에 미치는 영향을 실험적으로 확인하였으며, 모델의 각 레이어가 감지하는 언어적 패턴의 차이를 토큰 임베딩 벡터의 관점에서 분석하였습니다. 이를 통해 모델의 학습 과정과 일반화 성능을 개선할 수 있는 중요한 통찰을 제공합니다.
